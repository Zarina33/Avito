import os
import io
import torch
from flask import Flask, request, render_template_string
from PIL import Image
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoProcessor,
    AutoModelForImageTextToText
)

app = Flask(__name__)

# –ü—É—Ç–∏ –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞–º / —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º
vibe_model_dir = "/mnt/data/avito/vibe/models"
vibe_tokenizer_dir = "/mnt/data/avito/vibe/tokenizers"

vision_snapshot_dir = "/mnt/data/avito/vision/models/models--AvitoTech--avision/snapshots/def8375a2aa67643348ffd93143691410576663f"

# –ó–∞–≥—Ä—É–∑–∫–∞ Avibe (—Ç–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å)
tokenizer_avibe = AutoTokenizer.from_pretrained(
    "AvitoTech/avibe",
    cache_dir=vibe_tokenizer_dir,
    local_files_only=True
)
model_avibe = AutoModelForCausalLM.from_pretrained(
    "AvitoTech/avibe",
    cache_dir=vibe_model_dir,
    torch_dtype="auto",
    device_map="auto",
    local_files_only=True
)

# –ó–∞–≥—Ä—É–∑–∫–∞ Avision (–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä + –º–æ–¥–µ–ª—å)
processor_avision = AutoProcessor.from_pretrained(
    vision_snapshot_dir,
    local_files_only=True
)
model_avision = AutoModelForImageTextToText.from_pretrained(
    vision_snapshot_dir,
    torch_dtype="auto",
    device_map="auto",
    local_files_only=True
)

HTML = """
<!doctype html>
<html>
<head>
  <title>Avibe + Avision</title>
  <style>
    body { font-family: Arial, sans-serif; padding: 20px; }
    .section { margin-bottom: 40px; }
    textarea, input[type=text] { width: 60%; }
  </style>
</head>
<body>
  <h1>Avibe & Avision Demo</h1>
  
  <div class="section">
    <h2>üó£ Avibe (—Ç–µ–∫—Å—Ç–æ–≤—ã–π —á–∞—Ç)</h2>
    <form method="post" action="/avibe">
      <textarea name="prompt" rows="4" cols="60">–ü—Ä–∏–≤–µ—Ç, –ø–æ–¥—Å–∫–∞–∂–∏ —Ä–µ—Ü–µ–ø—Ç –±–æ—Ä—â–∞</textarea><br><br>
      <button type="submit">–û—Ç–ø—Ä–∞–≤–∏—Ç—å</button>
    </form>
  </div>
  
  <div class="section">
    <h2>üñº Avision (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ + –∞–Ω–∞–ª–∏–∑)</h2>
    <form method="post" action="/avision" enctype="multipart/form-data">
      <input type="file" name="image" accept="image/*"><br><br>
      <input type="text" name="prompt2" value="–û–ø–∏—à–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ –∏ —Å–∫–∞–∂–∏, —á—Ç–æ –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –ø—Ä–æ–¥–∞—Ç—å"><br><br>
      <button type="submit">–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å</button>
    </form>
  </div>

  {% if result %}
    <div class="section">
      <h2>–†–µ–∑—É–ª—å—Ç–∞—Ç:</h2>
      <pre>{{ result }}</pre>
      {% if image_data %}
        <img src="data:image/png;base64,{{ image_data }}" alt="Uploaded image" style="max-width:400px;" />
      {% endif %}
    </div>
  {% endif %}
  
</body>
</html>
"""

from base64 import b64encode

@app.route("/", methods=["GET"])
def index():
    return render_template_string(HTML)

@app.route("/avibe", methods=["POST"])
def route_avibe():
    prompt = request.form.get("prompt", "")
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer_avibe.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer_avibe([text], return_tensors="pt").to(model_avibe.device)
    generated_ids = model_avibe.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7
    )
    input_len = inputs.input_ids.shape[1]
    gen_ids = generated_ids[:, input_len:]
    response = tokenizer_avibe.decode(gen_ids[0], skip_special_tokens=True)
    return render_template_string(HTML, result=response, image_data=None)

@app.route("/avision", methods=["POST"])
def route_avision():
    prompt2 = request.form.get("prompt2", "")
    file = request.files.get("image")
    if not file:
        return render_template_string(HTML, result="No image uploaded", image_data=None)
    
    image_bytes = file.read()
    img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ë–õ–û–ö: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –ø–æ–¥—Ö–æ–¥, —á—Ç–æ –∏ –≤ —Ä–∞–±–æ—á–µ–º –∫–æ–¥–µ
    # 1. –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è apply_chat_template
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": img},
                {"type": "text", "text": prompt2}
            ],
        }
    ]
    
    # 2. –ü—Ä–∏–º–µ–Ω—è–µ–º chat template –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    chat_text = processor_avision.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞, –ø–µ—Ä–µ–¥–∞–≤–∞—è –≥–æ—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    inputs = processor_avision(
        text=[chat_text],  # –ü–µ—Ä–µ–¥–∞–µ–º –≥–æ—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É —á–∞—Ç–∞
        images=img,        # –ü–µ—Ä–µ–¥–∞–µ–º –æ–±—ä–µ–∫—Ç PIL.Image
        return_tensors="pt",
        padding=True
    ).to(model_avision.device)
    
    generated_ids = model_avision.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7
    )
    
    # –£–¥–∞–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞
    input_ids_len = inputs.input_ids.shape[1]
    generated_text_ids = generated_ids[:, input_ids_len:]
    response = processor_avision.batch_decode(generated_text_ids, skip_special_tokens=True)[0]
    
    img_data = b64encode(image_bytes).decode('utf-8')
    return render_template_string(HTML, result=response, image_data=img_data)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8085)
