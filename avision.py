import os
import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
import warnings

warnings.filterwarnings("ignore", category=UserWarning)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–≤–æ–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∫–µ—à–∞
# –ù–∞–ø—Ä–∏–º–µ—Ä:
root_cache_dir = "/home/zarina/Work/BakaiMarket/Avito/vision"
models_cache_dir = os.path.join(root_cache_dir, "models")
hub_cache_dir = os.path.join(root_cache_dir, "hub")
datasets_cache_dir = os.path.join(root_cache_dir, "datasets")

os.makedirs(models_cache_dir, exist_ok=True)
os.makedirs(hub_cache_dir, exist_ok=True)
os.makedirs(datasets_cache_dir, exist_ok=True)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è, —á—Ç–æ–±—ã Transformers / hub –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –Ω–∞—à–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.environ["HF_HOME"] = root_cache_dir
os.environ["TRANSFORMERS_CACHE"] = models_cache_dir
os.environ["HF_DATASETS_CACHE"] = datasets_cache_dir

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ---
model_id = "AvitoTech/avision"
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"‚úÖ –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")

# –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
local_image_path = "/home/zarina/Work/BakaiMarket/Avito/car5.jpeg"

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ (—Å —É–∫–∞–∑–∞–Ω–∏–µ–º cache_dir) ---
print(f"\n‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_id} –≤ {models_cache_dir} ...")
processor = AutoProcessor.from_pretrained(model_id, cache_dir=models_cache_dir, local_files_only=False)

model = AutoModelForImageTextToText.from_pretrained(
    model_id,
    torch_dtype="auto",
    cache_dir=models_cache_dir,
    local_files_only=False
)
model.to(device)
print("üöÄ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ---
if not os.path.exists(local_image_path):
    print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {local_image_path}")
    img = Image.new('RGB', (400, 300), color='red')
else:
    try:
        img = Image.open(local_image_path).convert('RGB')
        print(f"üñºÔ∏è –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {local_image_path}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        img = Image.new('RGB', (400, 300), color='red')

# --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (–û–ë–ù–û–í–õ–Å–ù–ù–´–ô –ë–õ–û–ö) ---
prompt_text = "–û–ø–∏—à–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ –∏ —Å–∫–∞–∂–∏, —á—Ç–æ –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –ø—Ä–æ–¥–∞—Ç—å."

# 1. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è: —Å–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –¥–ª—è apply_chat_template
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": img},
            {"type": "text", "text": prompt_text}
        ],
    }
]

# 2. –í–†–£–ß–ù–£–Æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —á–∞—Ç-—Å—Ç—Ä–æ–∫—É, –∫–æ—Ç–æ—Ä—É—é –æ–∂–∏–¥–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
chat_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)


# 3. –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä, –ø–µ—Ä–µ–¥–∞–≤–∞—è –¢–û–õ–¨–ö–û –≥–æ—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É 'chat_text' –∏ –æ–±—ä–µ–∫—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
# –≠—Ç–æ –æ–±—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—É —Å –∞—Ä–≥—É–º–µ–Ω—Ç–æ–º `messages`.
inputs = processor(
    text=[chat_text],  # <--- –ü–µ—Ä–µ–¥–∞–µ–º –≥–æ—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
    images=img,        # <--- –ü–µ—Ä–µ–¥–∞–µ–º –æ–±—ä–µ–∫—Ç PIL.Image
    return_tensors="pt",
    padding=True
)
inputs = inputs.to(device)

# --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ ---
print("\n‚öôÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
generated_ids = model.generate(
    **inputs,
    max_new_tokens=256,
    do_sample=True,
    temperature=0.7
)

# –£–¥–∞–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–¥–ª—è —á–∏—Å—Ç–æ—Ç—ã)
input_ids_len = inputs.input_ids.shape[1]
generated_text_ids = generated_ids[:, input_ids_len:]
response = processor.batch_decode(generated_text_ids, skip_special_tokens=True)[0]

# --- –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ---
print("\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç ---")
print(f"–ó–∞–ø—Ä–æ—Å: {prompt_text}")
print(f"–û—Ç–≤–µ—Ç: \n{response}")

# –û—á–∏—Å—Ç–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∫–µ—à–∏ –ø–∞–º—è—Ç–∏
del model
del processor
if torch.cuda.is_available():
    torch.cuda.empty_cache()
